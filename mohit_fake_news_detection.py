# -*- coding: utf-8 -*-
"""Mohit_fake_news_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13XjCyIE7C9sYv2RnG8fx42XXbUZQiFFC
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import matplotlib.pyplot as plt
import itertools
from sklearn import svm
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn import metrics
import spacy
from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS
import string
import re
import nltk
import collections
from nltk.corpus import stopwords
from sklearn.feature_extraction import DictVectorizer
from sklearn.pipeline import Pipeline, FeatureUnion
from empath import Empath
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import pickle
from sklearn import svm
from sklearn.linear_model import  LogisticRegression
import seaborn as sn

pip install empath

df = pd.read_csv('/content/konbert-export-1aea7fa165084.csv')
#df1=df.dropna(axis=0)
df.loc[df['is_sarcastic']== 0, 'Label'] = 'REAL'
df.loc[df['is_sarcastic']== 1, 'Label'] = 'FAKE'
df.columns
df['Label'].value_counts()
#print(df1.shape)

#Dropping the column URLs from the table
df.drop(['article_link'], axis = 1, inplace = True)
df.columns

df['Label'].value_counts().plot(kind = 'bar')

df['headline_length'] = [len(str(a)) for a in df['headline']]
df['headline_length'].describe()

df["Text"] = df["headline"].map(str)
y = df.Label
y = y.astype('str')
X_train, X_test, Y_train, Y_test = train_test_split(df['Text'],y, test_size=0.25)
X_train
Y_test
Y_train

#Tf-idf Bigrams
#Initialize the `tfidf_vectorizer` 
tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2)) 

# Fit and transform the training data 
tfidf1_train = tfidf_vectorizer.fit_transform(X_train.astype('str')) 

# Transform the test set 
tfidf1_test = tfidf_vectorizer.transform(X_test.astype('str'))

pickle.dump(tfidf1_train, open("tfidf1_train.pickle", "wb"))

pickle.dump(tfidf1_test, open("tfidf1_test.pickle", "wb"))

#Top 10 tfidf bigrams 
tfidf_vectorizer.get_feature_names()[-10:]

tfidf1_train

#Confusion Matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix')

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

clf = MultinomialNB()
clf.fit(tfidf1_train, Y_train)
pickle.dump(clf, open('tfidf_nb', 'wb'))
pred = clf.predict(tfidf1_test)
score = metrics.accuracy_score(Y_test, pred)
print("Accuracy with Multinomial Naive Bayes:   %0.3f" % score)

cm = metrics.confusion_matrix(Y_test, pred, labels=['FAKE', 'REAL'])
plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])

clf = GradientBoostingClassifier()
clf.fit(tfidf1_train, Y_train)
pickle.dump(clf, open('tfidf_gb', 'wb'))
#model = pickle.load(open('tfidf_gb', 'rb'))
pred = clf.predict(tfidf1_test)
score = metrics.accuracy_score(Y_test, pred)
print("Accuracy with Gradient Boosting:   %0.3f" % score)

cm = metrics.confusion_matrix(Y_test, pred, labels=['FAKE', 'REAL'])
plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])

clf = RandomForestClassifier()
clf.fit(tfidf1_train, Y_train)
pickle.dump(clf, open('tfidf_rf', 'wb'))
pred = clf.predict(tfidf1_test)
score = metrics.accuracy_score(Y_test, pred)
print("Accuracy with RandomForestClassifier:   %0.3f" % score)

cm = metrics.confusion_matrix(Y_test, pred, labels=['FAKE', 'REAL'])
plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])

clf = LogisticRegression()
clf.fit(tfidf1_train, Y_train)
pickle.dump(clf,open('tfidf_lr','wb'))
pred = clf.predict(tfidf1_test)
score = metrics.accuracy_score(Y_test, pred)
print("Accuracy with Logistic Regression: %0.3f" % score)

cm = metrics.confusion_matrix(Y_test, pred, labels=['FAKE', 'REAL'])
plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])

clf = svm.LinearSVC()
clf.fit(tfidf1_train, Y_train)
pickle.dump(clf,open('tfidf_svm','wb'))
pred = clf.predict(tfidf1_test)
score = metrics.accuracy_score(Y_test, pred)
print("Accuracy with svm: %0.3f" % score)

cm = metrics.confusion_matrix(Y_test, pred, labels=['FAKE', 'REAL'])
plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])

#Generating the POS tags for all the articles and adding a new column by replacing text with their POS tags
nlp = spacy.load('en_core_web_sm')
x = []
df["Text"] = df["headline"].map(str)
for text in df['Text']:
    text_new = []
    doc = nlp(text)
    for token in doc:
        text_new.append(token.pos_)
    txt = ' '.join(text_new)
    x.append(txt)
df['Text_pos'] = x
df.to_pickle('newdata.pkl')

y = df.Label
y = y.astype('str')
x_train, x_test, y_train, y_test = train_test_split(df['Text_pos'],y, test_size=0.25)
x_train

#Initialize the `tfidf_vectorizer` 
tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2)) 

# Fit and transform the training data 
tfidf_train = tfidf_vectorizer.fit_transform(x_train.astype('str')) 

# Transform the test set 
tfidf_test = tfidf_vectorizer.transform(x_test.astype('str'))

pickle.dump(tfidf_train, open("tfidf_train.pickle", "wb"))

pickle.dump(tfidf_test, open("tfidf_test.pickle", "wb"))

tfidf_vectorizer.get_feature_names()[-10:]

tfidf_train

clf = MultinomialNB()
clf.fit(tfidf_train, y_train)
pickle.dump(clf, open('pos_nb', 'wb'))
pred = clf.predict(tfidf_test)
score = metrics.accuracy_score(y_test, pred)
print("Accuracy with Multinomial Naive Bayes:   %0.3f" % score)

cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])
plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])

clf = RandomForestClassifier()
clf.fit(tfidf_train, y_train)
pickle.dump(clf, open('pos_rf', 'wb'))
pred = clf.predict(tfidf_test)
score = metrics.accuracy_score(y_test, pred)
print("Accuracy with RandomForestClassifier:   %0.3f" % score)

cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])
plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])

clf = GradientBoostingClassifier()
clf.fit(tfidf_train, y_train)
pickle.dump(clf, open('pos_gb', 'wb'))
pred = clf.predict(tfidf_test)
score = metrics.accuracy_score(y_test, pred)
print("Accuracy with Gradient Boosting:   %0.3f" % score)

cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])
plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])

clf = LogisticRegression()
clf.fit(tfidf_train, y_train)
pickle.dump(clf,open('pos_lr','wb'))
pred = clf.predict(tfidf_test)
score = metrics.accuracy_score(Y_test, pred)
print("Accuracy with Logistic Regression: %0.3f" % score)

cm = metrics.confusion_matrix(Y_test, pred, labels=['FAKE', 'REAL'])
plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])

clf = svm.LinearSVC()
clf.fit(tfidf_train, y_train)
pickle.dump(clf,open('pos_svm','wb'))
pred = clf.predict(tfidf_test)
score = metrics.accuracy_score(Y_test, pred)
print("Accuracy with svm: %0.3f" % score)

cm = metrics.confusion_matrix(Y_test, pred, labels=['FAKE', 'REAL'])
plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])

test_dataset , training_dataset = train_test_split(df, train_size=.25, test_size=.75, random_state=2)

def HeadlinetoWords(training_dataset):
    headline_train = list(training_dataset['headline'])
    wordList = []
    for count, words in enumerate(headline_train):
        wordList.append((words.split(' ')))
    return wordList

def exploreWordCount(wordList):
    wordsCount={}
    for words in wordList:
        for word in words:
            if word not in wordsCount:
                wordsCount[word]=1
            else:
                wordsCount[word]+=1
    return wordsCount

def top20words(wordCount):
    sortedWords = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)
    freqWords={}
    for index, word in enumerate(sortedWords):
        if index<19:
            freqWords[word[0]]=word[1]
    return freqWords

def least20words(wordCount):
    sortedWords = sorted(wordCount.items(), key=lambda x: x[1])
    leastfreqwords={}
    for index, word in enumerate(sortedWords):
        if index<19:
            leastfreqwords[word[0]] = word[1]
    return leastfreqwords

def plotFreqWords(freqWords, title):
    print("rand")
    words=freqWords.keys()
    counts=freqWords.values()
    y_pos = np.arange(len(words))
    plt.figure(figsize=(30,10))
    plt.bar(y_pos, counts, align='center', alpha=0.5,width=.8)
    plt.xticks(y_pos, words, rotation=20)
    plt.ylabel('Counts of words')
    plt.xlabel('Count in data')
    plt.title('Training Data {}'.format(title))
    plt.show()

def exploreDataBasedOnLabels(training_dataset):
    ###############################Exploring Training Data#######################
    #exploreData(df_train)
    train_wordList = HeadlinetoWords(training_dataset)
    trainWordsCount = exploreWordCount(train_wordList)    
    #allTrainWords= pd.DataFrame(data=[trainWordsCount]).T
    trainFreqWords = top20words(trainWordsCount)
    plotFreqWords(trainFreqWords, "Top 20 Frequent words in entire Data")
    leastTrainFreqWords = least20words(trainWordsCount)
    plotFreqWords(leastTrainFreqWords, "Least 20 Frequent words in entire Data")
        #Get Indices of Sarcastic and Non sarcastic classes
    is_sarcastic_Indices = training_dataset.index[training_dataset['is_sarcastic']==1].tolist()
    non_sarcastic_Indices = training_dataset.index[training_dataset['is_sarcastic']==0].tolist()
    
    df_training_Sarcastic = training_dataset.loc[is_sarcastic_Indices]
    df_training_NonSarcastic = training_dataset.loc[non_sarcastic_Indices]
    
    train_wordList_nonSarc = HeadlinetoWords(df_training_NonSarcastic)
    train_wordList_Sarc = HeadlinetoWords(df_training_Sarcastic)
    
    #explore length of headlines in sarcastic and non sarcastic
    length_of_Headlines_Sarc=[len(headline) for headline in train_wordList_Sarc]
    length_of_Headlines_NonSarc=[len(headline) for headline in train_wordList_nonSarc]
    #category[{'sar':value,'non':value}]
    category=['non_sarc']*len(length_of_Headlines_NonSarc)
    category.extend(['sarc']*len(length_of_Headlines_Sarc))
    lengthCategory = []
    lengthCategory.extend(length_of_Headlines_NonSarc)
    lengthCategory.extend(length_of_Headlines_Sarc)
    df=pd.DataFrame(data={'category':category,'Length':lengthCategory})
    sn.boxplot(x='Length',y='category',data=df)
    plt.title("Box-Plot describing the length of headline in each class")
    plt.show()
    trainWordsCount_nonSarc = exploreWordCount(train_wordList_nonSarc)
    trainWordsCount_Sarc = exploreWordCount(train_wordList_Sarc)

    trainFreqWords_nonSarc = top20words(trainWordsCount_nonSarc)
    trainFreqWords_Sarc = top20words(trainWordsCount_Sarc)
    
    plotFreqWords(trainFreqWords_nonSarc, "Top 20 Frequent Non-Sarcastic News")
    plotFreqWords(trainFreqWords_Sarc, "Top 20 Frequent Sarcastic News")
    
    trainLeastFreqWords_1 = least20words(trainWordsCount_nonSarc)
    trainLeastFreqWords_2 = least20words(trainWordsCount_Sarc)
    plotFreqWords(trainLeastFreqWords_1, "Least 20 Frequent Non-Sarcastic News")
    plotFreqWords(trainLeastFreqWords_2, "Least 20 Frequent Sarcastic News")

exploreDataBasedOnLabels(training_dataset)